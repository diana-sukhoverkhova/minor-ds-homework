{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "X-ZEdlEGAXTD",
        "hlIITUk0AsmS",
        "vJoXiCWI7VF5",
        "WwKE57YZ1Hzn",
        "HvCAL3qGDByj",
        "jFdy3lUFDsOr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHgmxWG_7lnE"
      },
      "source": [
        "# Введение в анализ данных\n",
        "## НИУ ВШЭ, 2019-2020 учебный год\n",
        "\n",
        "### Домашнее задание №3\n",
        "\n",
        "Задание выполнил(а): Суховерхова Диана\n",
        "\n",
        "### Общая информация\n",
        "Дата выдачи: \n",
        "\n",
        "Дедлайн: \n",
        "\n",
        "### О задании\n",
        "\n",
        "В этом домашнем задании вы будете работать с линейной классификацией, попрактикуетесь на реальной задаче классификации текстов.\n",
        "\n",
        "Для решения этого домашнего задания намного удобнее будет использовать Colab, так как данных много.\n",
        "\n",
        "### Оценивание и штрафы\n",
        "\n",
        "За сдачу задания позже срока на итоговую оценку за задание накладывается штраф в размере 1 балл в день, но получить отрицательную оценку нельзя.\n",
        "\n",
        "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов.\n",
        "\n",
        "### Формат сдачи\n",
        "Загрузка файлов с решениями происходит в системе Anytask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztx03xvr9T95"
      },
      "source": [
        "### Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVrrwTJNjuDt"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VMchexbjjTh",
        "outputId": "96c3a000-9ce2-432e-f61e-e2bab44a0c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "source": [
        "# Датасет можно скачать здесь\n",
        "\n",
        "!wget https://www.dropbox.com/s/tg55q9mrziroyrs/train_subset.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-27 17:37:59--  https://www.dropbox.com/s/tg55q9mrziroyrs/train_subset.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:6018:1::a27d:301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/tg55q9mrziroyrs/train_subset.csv [following]\n",
            "--2020-04-27 17:37:59--  https://www.dropbox.com/s/raw/tg55q9mrziroyrs/train_subset.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc0caa39599689728e1ce8888719.dl.dropboxusercontent.com/cd/0/inline/A2omifqxf7oAtbDCyI4Sduf0YfhxHWvRvDjSmPwSsvrSjXSW8PZ37X7_eiF5qJSsJns9nejX8LyAEXN7Ce4gtfyc7f_dTGPxQ7XCvZRHVEL6uMPqJcGsIvw2yVnWo6AKzKk/file# [following]\n",
            "--2020-04-27 17:37:59--  https://uc0caa39599689728e1ce8888719.dl.dropboxusercontent.com/cd/0/inline/A2omifqxf7oAtbDCyI4Sduf0YfhxHWvRvDjSmPwSsvrSjXSW8PZ37X7_eiF5qJSsJns9nejX8LyAEXN7Ce4gtfyc7f_dTGPxQ7XCvZRHVEL6uMPqJcGsIvw2yVnWo6AKzKk/file\n",
            "Resolving uc0caa39599689728e1ce8888719.dl.dropboxusercontent.com (uc0caa39599689728e1ce8888719.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc0caa39599689728e1ce8888719.dl.dropboxusercontent.com (uc0caa39599689728e1ce8888719.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19213119 (18M) [text/plain]\n",
            "Saving to: ‘train_subset.csv.7’\n",
            "\n",
            "train_subset.csv.7  100%[===================>]  18.32M  93.7MB/s    in 0.2s    \n",
            "\n",
            "2020-04-27 17:38:00 (93.7 MB/s) - ‘train_subset.csv.7’ saved [19213119/19213119]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvXKae8q9nn-"
      },
      "source": [
        "### Данные\n",
        "\n",
        "Мы имеем дело с данными с торговой платформы Avito.\n",
        "Для каждого товара представлены следующие параметры:\n",
        " - title\n",
        " - description\n",
        " - Category_name\n",
        " - Category\n",
        "\n",
        "Имеется информация об объектах 50 классов.\n",
        "Задача: по новым объектам (title, description) предсказать Category.\n",
        "(Очевидно, что параметр Category_name для предсказания классов использовать нельзя)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqEuoDhqNgoa",
        "outputId": "f528ade1-d5cb-44df-f1de-a6038ff5c893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "data = pd.read_csv(\"train_subset.csv\", index_col='id')\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>description</th>\n",
              "      <th>Category_name</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>382220</th>\n",
              "      <td>Прихожая</td>\n",
              "      <td>В хорошем состоянии. Торг</td>\n",
              "      <td>Мебель и интерьер</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397529</th>\n",
              "      <td>Кордиант 215/55/16 Летние</td>\n",
              "      <td>Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...</td>\n",
              "      <td>Запчасти и аксессуары</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584569</th>\n",
              "      <td>Стол</td>\n",
              "      <td>Стол, 2 рабочих места . Стол серого цвета, в д...</td>\n",
              "      <td>Мебель и интерьер</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2513100</th>\n",
              "      <td>Комбинезон</td>\n",
              "      <td>Размер-42/44</td>\n",
              "      <td>Одежда, обувь, аксессуары</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1091886</th>\n",
              "      <td>Ветровка</td>\n",
              "      <td>На 2 года</td>\n",
              "      <td>Детская одежда и обувь</td>\n",
              "      <td>29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             title  ... Category\n",
              "id                                  ...         \n",
              "382220                    Прихожая  ...       20\n",
              "397529   Кордиант 215/55/16 Летние  ...       10\n",
              "584569                        Стол  ...       20\n",
              "2513100                 Комбинезон  ...       27\n",
              "1091886                   Ветровка  ...       29\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg8iPp7fiwGh",
        "outputId": "838ff941-7974-4218-9e4f-26611c73c469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1hvzAMETU2d"
      },
      "source": [
        "X = data[['title', 'description']].to_numpy()\n",
        "y = data['Category'].to_numpy()\n",
        "\n",
        "del data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMYU7zZw_cw-"
      },
      "source": [
        "Сразу разделим выборку на train и test.\n",
        "Никакие данные из test для обучения использовать нельзя!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fia4_3vNprp"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDR8LtTJUIGt",
        "outputId": "8727d544-e109-44d8-c33f-3f3766166b21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "source": [
        "X_train[:5]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Сапоги 46 размер новые', 'Сапоги 46 размер новые'],\n",
              "       ['Светильники потолочный swarovski',\n",
              "        'светильники потолочные swarovski 6 штук , цена за штуку. В эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
              "       ['iPhone 7 plus 128GB Red красный в наличии',\n",
              "        '\\xa0/\\n/\\n Данная цена только для подписчиков Instagram: iQmac/\\n/\\n Новый красный айфон 7 Plus в наличии это элегантный и мощный смартфон, который готов в полной мере раскрыть новые возможности iOS 10. Аппарат с 4-ядерным процессором А10 и 3 ГБ ОЗУ с легкостью решает самые ресурсоемкие задачи, позволяя наслаждаться быстродействием «тяжелых» приложений и игр на 5,5-дюймовом дисплее. Аппарат получил экран, как у iPad Pro, так что картинка теперь соответствует кинематографическому стандарту.'],\n",
              "       ['Пион Ирис Ромашка рассада',\n",
              "        'Пион куст 500 р ( более 10 шт)/\\nСаженец/ корень 100р/\\nРастут у нас более 70 лет/\\nРозовые, бордовые и белые/\\nНа фото цветы 2018г/\\nП. Зубчаниновка/\\nлибо пл. Революции/\\nЕсть ирисы, ромашка, клубника, боярышник и ирга'],\n",
              "       ['Кофта', 'Состояние отличное']], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-ZEdlEGAXTD"
      },
      "source": [
        "### Токенизация (1 балл)\n",
        "\n",
        "\n",
        "Токенизация -- разбиение текста на мелкие части, которые можно обработать машинными методами.\n",
        "Можно использовать разные алгоритмы токенизации.\n",
        "Давайте пока остановимся на простом WordPunctTokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9VgNlZ1Qy3o",
        "outputId": "9f671282-3d30-49bb-fd9e-50c0452d5e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    return ' '.join(tokenizer.tokenize(text.lower()))\n",
        "\n",
        "\n",
        "text = 'Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...'\n",
        "print(\"before:\", text,)\n",
        "print(\"after:\", preprocess(text),)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...\n",
            "after: здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_RYBKC26o1X"
      },
      "source": [
        "__Задание:__ Токенизируйте title и description в train и test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5WO-7tJUvbs"
      },
      "source": [
        "tok = WordPunctTokenizer()\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    return ' '.join(tok.tokenize(text.lower()))\n",
        "\n",
        "\n",
        "for j in range(X_train.shape[0]):\n",
        "  X_train[j][0] = preprocess(X_train[j][0]) \n",
        "  X_train[j][1] = preprocess(X_train[j][1]) \n",
        "\n",
        "for j in range(X_test.shape[0]):\n",
        "  X_test[j][0] = preprocess(X_test[j][0]) \n",
        "  X_test[j][1] = preprocess(X_test[j][1]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kARGeJQwYTil",
        "outputId": "23b88aee-b059-48fd-fd1b-dbd18635874a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "X_train[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['сапоги 46 размер новые', 'сапоги 46 размер новые'],\n",
              "       ['светильники потолочный swarovski',\n",
              "        'светильники потолочные swarovski 6 штук , цена за штуку . в эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
              "       ['iphone 7 plus 128gb red красный в наличии',\n",
              "        '/ / данная цена только для подписчиков instagram : iqmac / / новый красный айфон 7 plus в наличии это элегантный и мощный смартфон , который готов в полной мере раскрыть новые возможности ios 10 . аппарат с 4 - ядерным процессором а10 и 3 гб озу с легкостью решает самые ресурсоемкие задачи , позволяя наслаждаться быстродействием « тяжелых » приложений и игр на 5 , 5 - дюймовом дисплее . аппарат получил экран , как у ipad pro , так что картинка теперь соответствует кинематографическому стандарту .'],\n",
              "       ['пион ирис ромашка рассада',\n",
              "        'пион куст 500 р ( более 10 шт )/ саженец / корень 100р / растут у нас более 70 лет / розовые , бордовые и белые / на фото цветы 2018г / п . зубчаниновка / либо пл . революции / есть ирисы , ромашка , клубника , боярышник и ирга'],\n",
              "       ['кофта', 'состояние отличное'],\n",
              "       ['1 - к квартира , 33 м² , 4 / 5 эт .',\n",
              "        'продаётся уютная , тёплая квартира в экологически - чистом районе города , рядом сосновый бор , всегда чистый воздух . дом 2004 г ., хорошие соседи , на площадке 2 - е квартиры , развитая инфраструктура , в шаговой доступности поликлиника , школа , тк « орбита », вещевой рынок . квартира в хорошем состоянии . подходит под ипотеку , долгов , обременений , перепланировке нет . в квартире натяжные потолки , в ванной комнате стены выполнены из влагостойких стеновых панелей . возможен обмен на квартиру в г . магнитогорске , торг .'],\n",
              "       ['платье новое 60 размера',\n",
              "        'платье 60 размера , новое , красивого темно синего цвета , из трикотажной ткани : вискоза 95 %, эластина 5 % . а - образного силуэта с рукавом 2 / 3 . длинна по спинке 113см .'],\n",
              "       ['ваз 2114 samara , 2007',\n",
              "        'продам ваз 2114 2007 г . в . в хорошем состоянии . / 2 владельца , птс оригинал . / машина в родной краске , в дтп никогда не была ,/ днище целое не ржавое . по ходовой нареканий нет , сел и поехал . / имеется музыка , сигнализация 2 комплекта ключей , птф , передние стеклоподъемники ./ небольшой торг при осмотре . / обмен не интересует .'],\n",
              "       ['наушники блутус',\n",
              "        'долго держат заряд 4 - 5 часов , можно и больше при средней громкости выжать из них . вкладыши .'],\n",
              "       ['пальто tommy hilfiger',\n",
              "        'состояние нового . промахнулась с размером . пальто до - 10 - 12 градусов . / возможна пересылка по почте']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDnDSWwFDwFo"
      },
      "source": [
        "assert X_train[10][1] == 'продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlIITUk0AsmS"
      },
      "source": [
        "### BOW (1.5 балла)\n",
        "\n",
        "Один из традиционных подходов -- построение bag of words.\n",
        "\n",
        "Метод состоит в следующем:\n",
        "\n",
        " - Составить словарь самых часто встречающихся слов в train data\n",
        " - Для каждого примера из train посчитать, сколько раз каждое слово из словаря в нём встречается\n",
        "\n",
        "\n",
        " В sklearn есть CountVectorizer, но в этом задании его использовать нельзя."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMKUttDWIF92"
      },
      "source": [
        "__Задание:__ Найдите k самых частых слов, отсортируйте их по убыванию частотности"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEVE_bzkRBx0"
      },
      "source": [
        "def bow_voc(text: np.array) -> list:\n",
        "    dct = {}\n",
        "    k = 10000\n",
        "    for j in range(text.shape[0]):\n",
        "       for sent in text[j]:\n",
        "         sent_spl = sent.split()\n",
        "         for word in sent_spl:\n",
        "          if word not in dct:\n",
        "             dct[word] = 0\n",
        "          else:\n",
        "             dct[word] += 1\n",
        "    list_d = list(dct.items())\n",
        "    list_d.sort(key=lambda i: i[1], reverse=True)\n",
        "    most_common = []\n",
        "    dct_with_ind = {}\n",
        "    for i in range(k):\n",
        "      most_common.append(list_d[i][0])\n",
        "      dct_with_ind[list_d[i][0]] = i\n",
        "    return most_common\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtEJl-IZCLyX"
      },
      "source": [
        "bow_vocabulary = bow_voc(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTs70ZxVbk0J"
      },
      "source": [
        "assert sorted(bow_vocabulary)[::200] == ['!', '12500', '270', '700', 'by', 'gh', 'michael', 'sonata', 'ø', 'аудиоподготовка', 'большим', 'веса', 'воспроизведения', 'габариты', 'гтд', 'джинсами', 'доступность', 'загрузки', 'зимней', 'использовался', 'квартала', 'коммуникации', 'кошки', 'лакированные', 'магазин', 'металл', 'мск', 'натуральным', 'носке', 'одному', 'отвечаем', 'пассат', 'плотно', 'покраску', 'постоянные', 'примеры', 'просьба', 'размещайте', 'репетитор', 'сантехник', 'сидения', 'современного', 'стала', 'схема', 'тон', 'удлиненная', 'фасад', 'цветами', 'шея', 'эту']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4awkhecbR9om"
      },
      "source": [
        "def text_to_bow(text: str) -> np.array:\n",
        "\n",
        "    \"\"\"\n",
        "    Возвращает вектор, где для каждого слова из most_common\n",
        "    указано количество его употреблений\n",
        "    \"\"\"   \n",
        "    words = text.split()\n",
        "    return np.asarray([words.count(word) for word in bow_vocabulary])\n",
        "    return res \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZnJT2JbdXA3"
      },
      "source": [
        "assert np.allclose(np.where(text_to_bow(\"сдаётся уютный , тёплый гараж для стартапов в ml\") != 0)[0],\n",
        "                   np.array([   1,    4,   12,  565,  866, 1601, 2539, 4063])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR_D8Fn4pudv"
      },
      "source": [
        "def items_to_bow(items: np.array) -> np.array:\n",
        "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
        "    # Давайте для начала попробуем строить bow только из description товара\n",
        "    # assert ниже написан для bow из description\n",
        "\n",
        "    ans = []\n",
        "    for index, prod in enumerate(items):\n",
        "      vect_bow = list(text_to_bow(prod[1]))\n",
        "      ans.append(vect_bow)\n",
        "    return np.array(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKdfMqbIetPA"
      },
      "source": [
        "assert np.allclose(np.where(items_to_bow([X_train[42]])[0] != 0),\n",
        "                   np.array([   0, 1, 2, 5, 6, 7, 12, 27, 41, 49, 110,\n",
        "                                189,  208,  221, 2032, 3052, 7179, 9568]),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwOZaEpMSQsZ"
      },
      "source": [
        "X_train_bow = items_to_bow(X_train)\n",
        "X_test_bow = items_to_bow(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJVLS8Fs3CeT"
      },
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJoXiCWI7VF5"
      },
      "source": [
        "### Логистическая регрессия и SVC (1 балл)\n",
        "\n",
        "\n",
        "Теперь описание каждого товара представлено, как точка в многомерном пространстве.\n",
        "Очень важно запомнить эту идею: дальше мы будем рассматривать разные способы перехода от текста к точке в пространстве.\n",
        "\n",
        "Для BOW каждое измерение в пространстве -- какое-то слово.\n",
        "Мы предполагаем, что текст описывается набором каких-то популярных слов, которые в нём встречаются, а близкие по смыслу тексты будут использовать одинаковые слова.\n",
        "\n",
        "Обучите логистическую регрессию и SVC с базовыми параметрами.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVjnkPbfB6Ip"
      },
      "source": [
        "from scipy.sparse import csr_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01Qu5D_Q3x6h"
      },
      "source": [
        "X_train_bow_sparsed = csr_matrix(X_train_bow)\n",
        "X_test_bow_sparsed = csr_matrix(X_test_bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFaeuCBFCB-d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky3HV1rTSS9L",
        "outputId": "11c6758b-7130-42ac-e245-dafb069427ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "bow_model = LogisticRegression(max_iter=100).fit(X_train_bow_sparsed, y_train)\n",
        "acc_sc = accuracy_score(bow_model.predict(X_test_bow_sparsed), y_test)\n",
        "print(acc_sc)\n",
        "\n",
        "assert accuracy_score(bow_model.predict(X_test_bow_sparsed), y_test) > 0.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7011111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRkef0CeCFQv"
      },
      "source": [
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c46ZT0lvF6T",
        "outputId": "29a5045f-b8d9-41ec-cb96-0ee168dd6d11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "bow_model = LinearSVC(max_iter=70).fit(X_train_bow_sparsed, y_train)\n",
        "print(accuracy_score(bow_model.predict(X_test_bow_sparsed), y_test))\n",
        "\n",
        "assert accuracy_score(bow_model.predict(X_test_bow_sparsed), y_test) > 0.68"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6835555555555556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zRLi57AzGYM"
      },
      "source": [
        "Результат: качество логистической регрессии - 70.1%, SVC - 68.4%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwKE57YZ1Hzn"
      },
      "source": [
        "### Модификация признаков (0.5 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewMlxQezL6Ax"
      },
      "source": [
        "Добавьте title товара в bow с произвольным весом, как изменится качество?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evqKo1r5L5BO"
      },
      "source": [
        "import random\n",
        "\n",
        "def items_to_bow_with_title(items: np.array) -> np.array:\n",
        "\n",
        "    ans = []\n",
        "    for index, prod in enumerate(items):\n",
        "      vect_bow = list(text_to_bow(prod[0]) * random.random())\n",
        "      ans.append(vect_bow)\n",
        "    return np.array(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrUNqfWZBz-T"
      },
      "source": [
        "X_train_bow_with_title = np.hstack((X_train_bow, items_to_bow_with_title(X_train)))\n",
        "X_test_bow_with_title = np.hstack((X_test_bow, items_to_bow_with_title(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZrb7Arrwlmo"
      },
      "source": [
        "X_train_bow_with_title_sparsed = csr_matrix(X_train_bow_with_title)\n",
        "X_test_bow_with_title_sparsed = csr_matrix(X_test_bow_with_title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85F3aV-jqQp6",
        "outputId": "273c4206-20d1-4412-dedd-ad8a28a76a89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "bow_model_title = LogisticRegression(max_iter=100).fit(X_train_bow_with_title_sparsed, y_train)\n",
        "acc_sc_title = accuracy_score(bow_model_title.predict(X_test_bow_with_title_sparsed), y_test)\n",
        "print(acc_sc_title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7332222222222222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blh4aNTjqegL",
        "outputId": "71e50efa-bbe9-4c25-cf75-f57d44702ec0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "bow_model_title = LinearSVC(max_iter=70).fit(X_train_bow_with_title_sparsed, y_train)\n",
        "print(accuracy_score(bow_model_title.predict(X_test_bow_with_title_sparsed), y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7288888888888889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFqIx_E6qsom"
      },
      "source": [
        "Результат: качество обеих моделей улучшилось (на 4% для логистической регрессии и на 6% для SVC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db4TyqzxMnby"
      },
      "source": [
        "Нормализуйте данные (`sklearn.preprocessing.normalize`) перед обучением. Что станет с качеством и почему?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8rVy6q1Mn4J"
      },
      "source": [
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "X_train_bow_normalize = normalize(X_train_bow_with_title_sparsed)\n",
        "X_test_bow_normalize = normalize(X_test_bow_with_title_sparsed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJHmg5iGzR69",
        "outputId": "04378e92-2592-4444-e2a3-846af357be87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "bow_model_title = LogisticRegression(max_iter=100).fit(X_train_bow_normalize, y_train)\n",
        "acc_sc_title = accuracy_score(bow_model_title.predict(X_test_bow_normalize), y_test)\n",
        "print(acc_sc_title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6224444444444445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efJI1K1UzmpE",
        "outputId": "2cd9b69c-4664-43f4-9eaa-672355b8737b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bow_model_title = LinearSVC(max_iter=70).fit(X_train_bow_normalize, y_train)\n",
        "print(accuracy_score(bow_model_title.predict(X_test_bow_normalize), y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUmOXKGV1-bx"
      },
      "source": [
        "Результат: качество логистической регрессии уменьшилось на 14%, а SVC - выросло на 4% (по сравнению с предыдущими моделями) \\\\\n",
        "Это объясняется тем, что в логистической регресии нормализация проводилась отдельно для обучающей и тестирующей выборок."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvCAL3qGDByj"
      },
      "source": [
        "### mystem (0.5) балла\n",
        "\n",
        "Попробуйте обучиться, используя токенизатор mystem. Сравните качество."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz38TqqRDY6-",
        "outputId": "c2d47c60-e49e-41e2-9cb1-af7dd7a56cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /bin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-27 11:11:21--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.244, 5.45.205.242, 5.45.205.243, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.244|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://cache-mskm906.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz [following]\n",
            "--2020-04-27 11:11:22--  http://cache-mskm906.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving cache-mskm906.cdn.yandex.net (cache-mskm906.cdn.yandex.net)... 5.45.220.16, 2a02:6b8:0:2002::17\n",
            "Connecting to cache-mskm906.cdn.yandex.net (cache-mskm906.cdn.yandex.net)|5.45.220.16|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16457938 (16M) [application/octet-stream]\n",
            "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz’\n",
            "\n",
            "mystem-3.0-linux3.1 100%[===================>]  15.70M  8.14MB/s    in 1.9s    \n",
            "\n",
            "2020-04-27 11:11:25 (8.14 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz’ saved [16457938/16457938]\n",
            "\n",
            "mystem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60oQ-6UgDcLF",
        "outputId": "8246ba4e-a078-4988-dd1f-8fad6d81873a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "pip install git+https://github.com/nlpub/pymystem3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/nlpub/pymystem3\n",
            "  Cloning https://github.com/nlpub/pymystem3 to /tmp/pip-req-build-7hy7m7pt\n",
            "  Running command git clone -q https://github.com/nlpub/pymystem3 /tmp/pip-req-build-7hy7m7pt\n",
            "Requirement already satisfied (use --upgrade to upgrade): pymystem3==0.2.0 from git+https://github.com/nlpub/pymystem3 in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pymystem3==0.2.0) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.2.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.2.0) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pymystem3==0.2.0) (2.8)\n",
            "Building wheels for collected packages: pymystem3\n",
            "  Building wheel for pymystem3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymystem3: filename=pymystem3-0.2.0-cp36-none-any.whl size=9921 sha256=7303eb6a5971903fc7f159351baf05d0dc61ac64b5b163543360be8c55596e2a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n3sk7ra5/wheels/7d/75/c2/216a594291dee680749ce12c60d16125cfe1f363059e7163dc\n",
            "Successfully built pymystem3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGvNHfVsDfhq"
      },
      "source": [
        "from pymystem3 import Mystem\n",
        "\n",
        "mystem = Mystem()\n",
        "\n",
        "def preprocess_mystem(text: str) -> str:\n",
        "    return ' '.join(mystem.lemmatize(text.lower()))\n",
        "\n",
        "\n",
        "for j in range(X_train.shape[0]):\n",
        "  X_train[j][0] = preprocess_mystem(X_train[j][0]) \n",
        "  X_train[j][1] = preprocess_mystem(X_train[j][1]) \n",
        "\n",
        "for j in range(X_test.shape[0]):\n",
        "  X_test[j][0] = preprocess_mystem(X_test[j][0]) \n",
        "  X_test[j][1] = preprocess_mystem(X_test[j][1]) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKMTE11S7TYW"
      },
      "source": [
        "bow_vocabulary_ms = bow_voc(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4GaEHkZ9w5L"
      },
      "source": [
        "def text_to_bow_ms(text: str) -> np.array:\n",
        "\n",
        "    \"\"\"\n",
        "    Возвращает вектор, где для каждого слова из most_common\n",
        "    указано количество его употреблений\n",
        "    \"\"\"   \n",
        "    words = text.split()\n",
        "    return np.asarray([words.count(word) for word in bow_vocabulary_ms])\n",
        "    return res \n",
        "\n",
        "\n",
        "def items_to_bow_ms(items: np.array) -> np.array:\n",
        "    \"\"\" Для каждого товара возвращает вектор его bow \"\"\"\n",
        "    \n",
        "    ans = []\n",
        "    for index, prod in enumerate(items):\n",
        "      vect_bow = list(text_to_bow_ms(prod[1]))\n",
        "      ans.append(vect_bow)\n",
        "    return np.array(ans)\n",
        "\n",
        "    \n",
        "def items_to_bow_with_title_ms(items: np.array) -> np.array:\n",
        "\n",
        "    ans = []\n",
        "    for index, prod in enumerate(items):\n",
        "      vect_bow = list(text_to_bow_ms(prod[0]) * random.random())\n",
        "      ans.append(vect_bow)\n",
        "    return np.array(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNHGh5U_7E21"
      },
      "source": [
        "X_train_bow_ms = items_to_bow_ms(X_train)\n",
        "X_test_bow_ms = items_to_bow_ms(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cL2yXTS6-c5"
      },
      "source": [
        "X_train_bow_with_title_ms = np.hstack((X_train_bow_ms, items_to_bow_with_title_ms(X_train)))\n",
        "X_test_bow_with_title_ms = np.hstack((X_test_bow_ms, items_to_bow_with_title_ms(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW71tcCr6wGU"
      },
      "source": [
        "X_train_bow_with_title_sparsed_ms = csr_matrix(X_train_bow_with_title_ms)\n",
        "X_test_bow_with_title_sparsed_ms = csr_matrix(X_test_bow_with_title_ms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnGFiDI5-lze",
        "outputId": "1226e6dc-e1af-4eef-b24b-447e5c8c9872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "bow_model_title = LogisticRegression(max_iter=100).fit(X_train_bow_with_title_sparsed_ms, y_train)\n",
        "acc_sc_title = accuracy_score(bow_model_title.predict(X_test_bow_with_title_sparsed_ms), y_test)\n",
        "print(acc_sc_title)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7582222222222222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQxSG_Td-rik",
        "outputId": "b1c938f8-dcdc-4674-acc8-df8e2b6453b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "bow_model_title = LinearSVC(max_iter=70).fit(X_train_bow_with_title_sparsed_ms, y_train)\n",
        "print(accuracy_score(bow_model_title.predict(X_test_bow_with_title_sparsed_ms), y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7541111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhRdyonaFZCf"
      },
      "source": [
        "Результат: качество обеих моделей улучшилось по сравнению с предыдущими (без нормализации данных): качество логистической регрессии выросло на 3%, SVC - также на 3%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXbsPtpfoB7m"
      },
      "source": [
        "### TF-IDF (1.5 балла)\n",
        "\n",
        "Не все слова полезны одинаково, давайте попробуем [взвесить](http://tfidf.com/) их, чтобы отобрать более полезные.\n",
        "\n",
        "\n",
        "> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
        "> \n",
        "> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
        "\n",
        "\n",
        "В sklearn есть TfidfVectorizer, но в этом задании его использовать нельзя."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yIeoic7o3ES"
      },
      "source": [
        "# Давайте для простоты считать один tf-idf для title и description.\n",
        "# Для каждого слова из bow_vocabulary нужно посчитать\n",
        "#   1. Сколько раз оно встретилось в title и description во всём X_train\n",
        "#   2. В тексте скольких товаров встретилось это слово\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "def count_tfidf(matr: np.array):\n",
        "  k = 10000\n",
        "  text = []\n",
        "  for i in range(matr.shape[0]):\n",
        "    words = (matr[i][0] + ' ' + matr[i][1]).split()\n",
        "    for word in words:\n",
        "      text.append(word)\n",
        "  most_comm = Counter(text)\n",
        "  most_common = most_comm.most_common(k)\n",
        "\n",
        "  texts = []\n",
        "  for i in range(matr.shape[0]):\n",
        "    hp = list(set((matr[i][0] + ' ' + matr[i][1]).split()))\n",
        "    texts.append(hp)\n",
        "  idf_most_com = [0] * k\n",
        "  for j in range(k):\n",
        "    word = most_common[j][0]\n",
        "    idf_most_com[j] += sum([1 for i in texts if word in i])\n",
        "  return (most_common, idf_most_com)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgzTcKn2V33I"
      },
      "source": [
        "count_arr = count_tfidf(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i5zFpD9rbtz"
      },
      "source": [
        "from math import log\n",
        "\n",
        "def text_to_tfidf(text: str) -> np.array:\n",
        "    \"\"\"\n",
        "    Возвращает вектор, где для каждого слова из most_common\n",
        "    указан tf-idf\n",
        "    \"\"\"\n",
        "    k = 10000\n",
        "    words = set(text.split())\n",
        "    ans = [0] * k\n",
        "    i = 0\n",
        "    for i in range(k):\n",
        "      tp = count_arr[0][i]\n",
        "      if tp[0] in words:\n",
        "        ans[i] += (tp[1] / len(count_arr[0])) * log(X_train.shape[0] / count_arr[1][i])\n",
        "      i += 1\n",
        "    return np.array(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDBfPE7FtzgM"
      },
      "source": [
        "def my_tfidf(matr: np.array):\n",
        "  res = []\n",
        "  for i in range(matr.shape[0]):\n",
        "    res.append(text_to_tfidf(matr[i][0] + ' ' + matr[i][1]))\n",
        "  return np.array(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvL9BH7DsJrv"
      },
      "source": [
        "# Нормализуйте данные\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "X_train_normalize = normalize(my_tfidf(X_train))\n",
        "X_test_normalize = normalize(my_tfidf(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YFA-8kE1RHk"
      },
      "source": [
        "### Модели на TF-IDF признаках (1 балл)\n",
        "\n",
        "Обучите логистическую регрессию и SVC, оцените качество (accuracy_score)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ULrXsF1m5sU"
      },
      "source": [
        "X_train_norm_sparsed = csr_matrix(X_train_normalize)\n",
        "X_test_norm_sparsed = csr_matrix(X_test_normalize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kt2mHb4O6xW",
        "outputId": "b09db183-7aed-4684-843f-4e55567959ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "bow_model = LogisticRegression(max_iter=100).fit(X_train_norm_sparsed, y_train)\n",
        "acc_sc = accuracy_score(bow_model.predict(X_test_norm_sparsed), y_test)\n",
        "print(acc_sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.42744444444444446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxUULODmlOiE",
        "outputId": "41b54680-c888-4634-afc6-d0d26bc84813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "bow_model = LinearSVC(max_iter=70).fit(X_train_norm_sparsed, y_train)\n",
        "print(accuracy_score(bow_model.predict(X_test_norm_sparsed), y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5575555555555556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMsfTTriCeOl"
      },
      "source": [
        "Результат: качество логистической регрессии составило 42.7%, а SVC - 55.6%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdy3lUFDsOr"
      },
      "source": [
        "### Hashing Vectorizer (0.5 балла)\n",
        "\n",
        "Попробуйте использовать `sklearn.feature_extraction.text.HashingVectorizer` для векторизации текстов.\n",
        "Обязательно оцените качество работы алгоритмов классификации с использованием новой векторизации."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y666HTrqDq1m"
      },
      "source": [
        " from sklearn.feature_extraction.text import HashingVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4Kpao4MmxZp"
      },
      "source": [
        "def vect(matr: np.array):\n",
        "  res = []\n",
        "  for i in range(matr.shape[0]):\n",
        "    res.append(matr[i][0] + ' ' + matr[i][1])\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5-gM8IQliFI"
      },
      "source": [
        "vectorizer = HashingVectorizer(n_features=10000)\n",
        "X_train_hash = vectorizer.transform(vect(X_train))\n",
        "X_test_hash = vectorizer.transform(vect(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXXQD4CIsNwg"
      },
      "source": [
        "X_train_hash_sparsed = csr_matrix(X_train_hash)\n",
        "X_test_hash_sparsed = csr_matrix(X_test_hash)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tnp_PPNDqXsQ",
        "outputId": "115a02a9-0e06-431a-fd09-592e4157adfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "bow_model = LogisticRegression(max_iter=100).fit(X_train_hash_sparsed, y_train)\n",
        "acc_sc = accuracy_score(bow_model.predict(X_test_hash_sparsed), y_test)\n",
        "print(acc_sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7208888888888889\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7_oeVDwubcQ",
        "outputId": "4b00ee63-27eb-4baf-d8f6-6cbf2dc7d8e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bow_model = LinearSVC(max_iter=70).fit(X_train_hash_sparsed, y_train)\n",
        "print(accuracy_score(bow_model.predict(X_test_hash_sparsed), y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7965555555555556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmRq55RRvELE"
      },
      "source": [
        "Результат: качество логистической регрессии - 72.1%, SVC - 79.7%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQZ61xSsTpZI"
      },
      "source": [
        "### Word Vectors (1 балл)\n",
        "\n",
        "Давайте попробуем другой подход -- кажому слову сопоставим какой-то эмбеддинг (вектор).\n",
        "\n",
        "Вектора будут небольшой размерности. Таким образом мы снизим количество параметров в модели.\n",
        "\n",
        "Вектора мы возьмём уже готовые (обученные на текстах их интернета), так что наша модель будет знать некоторую дополнительную информацию о внешнем мире."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T38J27NcYGx5",
        "outputId": "f047e7c4-4bfa-4205-db56-1ec118d63b99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "!wget https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-27 17:55:30--  https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.1, 2620:100:6018:1::a27d:301\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/0x7oxso6x93efzj/ru.tar.gz [following]\n",
            "--2020-04-27 17:55:30--  https://www.dropbox.com/s/raw/0x7oxso6x93efzj/ru.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1ee8c7bf7e8417329bc6b553e2.dl.dropboxusercontent.com/cd/0/inline/A2pUMw0lpN-nayREpYijUT6CyHPsYR6wiRf1QqMSZ4AveGOBG-BNDOkfmXO4RLiaOl2sm8VU_kkUQxmflu2bQbNATwYBgNtcVxkw6X-Rnbkrxg/file# [following]\n",
            "--2020-04-27 17:55:30--  https://uc1ee8c7bf7e8417329bc6b553e2.dl.dropboxusercontent.com/cd/0/inline/A2pUMw0lpN-nayREpYijUT6CyHPsYR6wiRf1QqMSZ4AveGOBG-BNDOkfmXO4RLiaOl2sm8VU_kkUQxmflu2bQbNATwYBgNtcVxkw6X-Rnbkrxg/file\n",
            "Resolving uc1ee8c7bf7e8417329bc6b553e2.dl.dropboxusercontent.com (uc1ee8c7bf7e8417329bc6b553e2.dl.dropboxusercontent.com)... 162.125.3.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc1ee8c7bf7e8417329bc6b553e2.dl.dropboxusercontent.com (uc1ee8c7bf7e8417329bc6b553e2.dl.dropboxusercontent.com)|162.125.3.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/A2rBV_uRiMSdS40da7pI9_p18aVLL3RmO0PUTn63USrrhGrJjHbsOLpVU1yHqeQ25dOC0RgsOqtubSN3Y7T1f9f8Xu0QcQPxqCRbRTkcLEwjm38Q0fc_Gwgd7Gu0EF_6J_VkNRXrPUSMQTc9rqUBdTzFShbil2JkNBeU8mUFqb6YWtz0zuWABv9l3xH5Opm_cWFsto7OYzFfGmpOQy5tPVLV-sFqXtSMgow2ihgyQz2DfyHwvXWsTxqb8eennwRqtCl1TE_DbUjSnhKtLglzu3oYBsJ110WLNaUTgWKsf0-L_SxI9Ma9BLpjFlulSiwwvV-Enii7gWf_HCsCVvMq80kg/file [following]\n",
            "--2020-04-27 17:55:31--  https://uc1ee8c7bf7e8417329bc6b553e2.dl.dropboxusercontent.com/cd/0/inline2/A2rBV_uRiMSdS40da7pI9_p18aVLL3RmO0PUTn63USrrhGrJjHbsOLpVU1yHqeQ25dOC0RgsOqtubSN3Y7T1f9f8Xu0QcQPxqCRbRTkcLEwjm38Q0fc_Gwgd7Gu0EF_6J_VkNRXrPUSMQTc9rqUBdTzFShbil2JkNBeU8mUFqb6YWtz0zuWABv9l3xH5Opm_cWFsto7OYzFfGmpOQy5tPVLV-sFqXtSMgow2ihgyQz2DfyHwvXWsTxqb8eennwRqtCl1TE_DbUjSnhKtLglzu3oYBsJ110WLNaUTgWKsf0-L_SxI9Ma9BLpjFlulSiwwvV-Enii7gWf_HCsCVvMq80kg/file\n",
            "Reusing existing connection to uc1ee8c7bf7e8417329bc6b553e2.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2399456034 (2.2G) [application/octet-stream]\n",
            "Saving to: ‘ru.tar.gz’\n",
            "\n",
            "ru.tar.gz           100%[===================>]   2.23G  22.4MB/s    in 87s     \n",
            "\n",
            "2020-04-27 17:56:59 (26.3 MB/s) - ‘ru.tar.gz’ saved [2399456034/2399456034]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfse4xVbgMIr",
        "outputId": "970d07b4-b957-4800-a02a-2732843a8054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "!tar xzfv /content/ru.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ru.bin\n",
            "ru.vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy2TXmQ2jZSY"
      },
      "source": [
        "import gensim\n",
        "from gensim.models.wrappers import FastText\n",
        "\n",
        "model = FastText.load_fasttext_format('ru.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H49QR_jhjmCa"
      },
      "source": [
        "# Эмбеддинг предложения -- сумма эмбеддингов токенов\n",
        "\n",
        "\n",
        "def sentence_embedding(sentence: str) -> np.array:\n",
        "    \"\"\"\n",
        "    Складывает вектора токенов строки sentence\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_dim = model['кек'].shape[0]\n",
        "    features = np.zeros([embedding_dim], dtype='float32')\n",
        "    \n",
        "    for word in sentence.split():\n",
        "        if word in model:\n",
        "            features += model[word]\n",
        "    \n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj6U_hjtlllV"
      },
      "source": [
        "assert np.allclose(sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml')[::50],\n",
        "                   np.array([ 0.08189847,  0.07249198, -0.15601222,  0.03782297,  0.09215296, -0.23092946]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tfhc-PFmGvu"
      },
      "source": [
        "# Обучите логистическую регрессию и SVM\n",
        "# Оцените качество (accuracy_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZXF0oOM3Mua"
      },
      "source": [
        "def sent_and_emb(matr: np.array):\n",
        "  res = []\n",
        "  for i in range(matr.shape[0]):\n",
        "    sent = matr[i][0] + ' ' + matr[i][1]\n",
        "    res.append(sentence_embedding(sent))\n",
        "  return np.array(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjffZhKJ3EvK"
      },
      "source": [
        "X_train_emb = sent_and_emb(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYr2EuaN5JdK"
      },
      "source": [
        "X_test_emb = sent_and_emb(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypRWR-uR5MqU"
      },
      "source": [
        "X_train_emb_sparsed = csr_matrix(X_train_emb)\n",
        "X_test_emb_sparsed = csr_matrix(X_test_emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhXesrrr237m",
        "outputId": "02fbd7bb-e55d-4210-f5e1-7c89aade1082",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "bow_model = LogisticRegression(max_iter=100).fit(X_train_emb_sparsed, y_train)\n",
        "acc_sc = accuracy_score(bow_model.predict(X_test_emb_sparsed), y_test)\n",
        "print(acc_sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5773333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zVVk8c53Amj",
        "outputId": "e366bca5-3900-4290-bd65-6712c4c3600d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "bow_model = LinearSVC(max_iter=70).fit(X_train_emb_sparsed, y_train)\n",
        "print(accuracy_score(bow_model.predict(X_test_emb_sparsed), y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5921111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrQFLGuD5w0a"
      },
      "source": [
        "Результат: качество логистической регрессии - 57.7%, SVC - 59.2%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVEdlFostSnX"
      },
      "source": [
        "### Что дальше?\n",
        "\n",
        "Решение каждого пункта 1 балл:\n",
        "\n",
        "1. N-Gram модели текстовой классификации\n",
        "\n",
        "2. Обучиться на полных данных (ссылка на них появится здесь)\n",
        "\n",
        "3. Поработать с другими эмбеддингами (word2vec, GloVe).\n",
        "\n",
        "4. Использовать Vowpal Wabbit вместо sklearn.\n",
        "\n",
        "5. Другие способы токенизации (pymorphy2, spaCy)\n",
        "\n",
        "\n",
        "Снабжайте код пояснениями и графиками.\n",
        "Обязательно необходимо написать вывод по каждому пункту, который вы реализуете."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyIh2PN3udhD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}